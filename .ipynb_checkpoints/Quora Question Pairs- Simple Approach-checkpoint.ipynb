{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Question Pairs Duplicate Check through Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import pandas for reading in csv file, numpy for math operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set= pd.read_csv('/Users/srikharpadmanabhan/Documents/machine-learninig-quora/quora-question-pairs/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure training_set is read in correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take just the questions from training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_questions=training_set[['question1','question2']]\n",
    "#training_set.head(3)\n",
    "training_questions.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions=pd.Series(training_questions['question1'].tolist()+training_questions['question2'].tolist()).astype(str)\n",
    "train_questions.str.replace('?',' ')\n",
    "train_questions.str.replace('.',' ')\n",
    "train_questions.str.replace('!',' ')\n",
    "train_questions.str.replace('(',' ')\n",
    "train_questions.str.replace(')',' ')\n",
    "train_questions.str.replace('[',' ')\n",
    "train_questions.str.replace(']',' ')\n",
    "train_questions.str.replace('-',' ')\n",
    "train_questions.str.replace('_',' ')\n",
    "train_questions.str.replace('/',' ')\n",
    "train_questions.str.replace('#',' ')\n",
    "train_questions.str.replace('$',' ')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def set_weights(count,min_count):\n",
    "    if(count < min_count ):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1/(count+5000)\n",
    "\n",
    "word_collection=(\" \".join(train_questions)).lower().split()\n",
    "counts=Counter(word_collection)\n",
    "weights={word: set_weights(count,3) for word, count in counts.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that return the compatiblity of the two questions. Exclude words that are common in english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_word_match(row):\n",
    "    notWords=set(stopwords.words(\"english\"))\n",
    "    question1_words = {}\n",
    "    question2_words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in notWords:\n",
    "            question1_words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in notWords:\n",
    "            question2_words[word] = 1\n",
    "    if len(question1_words) == 0 or len(question2_words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in question1_words.keys() if w in question2_words] + [weights.get(w, 0) for w in question2_words.keys() if w in question1_words]\n",
    "    total_weights = [weights.get(w, 0) for w in question1_words] + [weights.get(w, 0) for w in question2_words]\n",
    "    \n",
    "    matchability = np.sum(shared_weights)/np.sum(total_weights)\n",
    "    if np.sum(total_weights)==0:\n",
    "        return 0.3\n",
    "    else:\n",
    "        return matchability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_word_match=training_set.apply(question_word_match, axis=1, raw=True)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(training_set['is_duplicate'], train_word_match, pos_label=1)\n",
    "print(fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_curve='ROC curve (area = ' +str(roc_auc_score(training_set['is_duplicate'], train_word_match))\n",
    "plt.plot(fpr,tpr,color='blue',label=label_curve)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve: Weighted Words')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.fill_between(fpr,tpr,color='navajowhite')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(10, True, 1)\n",
    "\n",
    "for train, test in kfold.split(training_questions):\n",
    "    train_questions1=pd.Series(training_questions.loc[train,['question1']].tolist()+training_questions.loc[train,['question2']].tolist()).astype(str)\n",
    "    train_questions1.str.replace('?',' ')\n",
    "    train_questions1.str.replace('.',' ')\n",
    "    train_questions1.str.replace('!',' ')\n",
    "    train_questions1.str.replace('(',' ')\n",
    "    train_questions1.str.replace(')',' ')\n",
    "    train_questions1.str.replace('[',' ')\n",
    "    train_questions1.str.replace(']',' ')\n",
    "    train_questions1.str.replace('-',' ')\n",
    "    train_questions1.str.replace('_',' ')\n",
    "    train_questions1.str.replace('/',' ')\n",
    "    train_questions1.str.replace('#',' ')\n",
    "    train_questions1.str.replace('$',' ')\n",
    "    \n",
    "    \n",
    "    \n",
    "    word_collection1=(\" \".join(train_questions.loc[train])).lower().split()\n",
    "    counters=Counter(word_collection1)\n",
    "    weights={word: set_weights(count,3) for word, count in counters.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
